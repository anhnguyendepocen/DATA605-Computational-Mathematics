---
title: "DATA605 Final Project"
author: "Javern Wilson"
date: "12/15/2019"
output: 
 html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
    theme: cosmo
    highlight: espresso
---

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(gridExtra)
library(kableExtra)
library(psych)
library(corrplot)
library(matrixcalc)
library(MASS)
library(Rmisc)
library(mice)
library(VIM)
library(broom)
```

# Problem 1

Using R, generate a random variable X that has 10,000 random uniform numbers from 1 to N, where N can be any number of your choosing greater than or equal to 6.  Then generate a random variable Y that has 10,000 random normal numbers with a mean of $\mu = \sigma = (N+1)/2$.

```{r}
set.seed(123)
N <- 7
mn <- sd <- (N + 1)/2
X <- runif(10000, 1, N)
Y <- rnorm(10000, mean = mn, sd=sd)
```


## Probability

Calculate as a minimum the below probabilities a through c.  Assume the small letter "x" is estimated as the median of the X variable, and the small letter "y" is estimated as the 1st quartile of the Y variable.  Interpret the meaning of all probabilities.

```{r}
x <- median(X)
find_quant <- data.frame(quantile(Y)) #get the quantiles for Y
y <- find_quant$quantile.Y.[2] #get the first quantile
```


  -  $P(X>x | X>y)$		
  
```{r}
sum((((X > x ) & (X > y )))/10000) / (sum(X > y)/10000)
```

There is a `53.1`% chance that $X$ is greater than its median given $X$ is greater than the first quantile of $Y$. 

  -  $P(X>x, Y>y)$
  
```{r}
sum((X > x ) * (Y > y ))/10000
```

Results show that there is `37.56`% probability that $X$ is greater than its median and $Y$ is greater than its first quantile

  -  $P(X<x | X>y)$		
  
```{r}
sum((((X < x ) & (X > y )))/10000) / (sum(X > y)/10000)
```

Here is the probability that $X$ is less than its median and $X$ is greater than the first quantile of $Y$ is `46.9`%.


#### Investigate whether $P(X>x \ and \ Y>y)=P(X>x)P(Y>y)$ by building a table and evaluating the marginal and joint probabilities.

##### Contingency Table 1

```{r}
#Marginal
Xgx <- sum(X > x)/10000
Xlx <- sum(X < x)/10000
Ygy <- sum(Y > y)/10000
Yly <- sum(Y < y)/10000

#joint
Xgy <- sum(X>x & Y>y)/10000
Xgly <- sum(X>x & Y<y)/10000
Xly <- sum(X<x & Y<y)/10000
Xlgy <-sum(X<x & Y>y)/10000

row1 <- c(Xgx, Xgy, 0, Xgly)
row2 <- c(Xgy, Ygy, Xlgy, 0)
row3 <- c(0, Xlgy, Xgx, Xly)
row4 <- c(Xgly, 0, Xly, Yly)
contingency_table1 <- data.frame(rbind(row1, row2, row3, row4))
colnames(contingency_table1) <- c("X>x", "Y>y", "X<x", "Y<y")
row.names(contingency_table1) <- c("X>x", "Y>y", "X<x", "Y<y")

kable(contingency_table1) %>% kable_styling(bootstrap_options = c("striped", "responsive"), full_width = F, position = "left")

```

Based on the table above:

$P(X>x \ and \ Y>y) = 0.3756$

$P(X>x) = 0.5$

$P(Y>y) = 0.75$

```{r}
0.75 * 0.5
```

The two probabilities are the same.


Check to see if independence holds by using Fisher’s Exact Test and the Chi Square Test.  What is the difference between the two? Which is most appropriate?

##### Contingency Table 2

```{r}
jm_df <- cbind(X, Y)
contingency_table2 <- prop.table(table(jm_df[,1]>x, jm_df[,2]>y))

row.names(contingency_table2) <- c("X<x", "X>x")

colnames(contingency_table2) <- c("Y<y", "Y>y")

kable(contingency_table2) %>% kable_styling("striped", full_width = F, position = "left")
```


### Fisher's Exact Test

```{r}
fisher.test(contingency_table2)
```



### Chi Square Test

```{r}
chisq.test(contingency_table2)
```

For both tests the P-value is greater than 0.05, so we believe the variables are independent.

Fisher’s exact test is best used for testing independence that is typically used only for 2×2 contingency table while the Chi-squared is best for larger samples. In our case, Fisher's Exact test is more appropriate.


***


# Problem 2

You are to register for Kaggle.com (free) and compete in the *House Prices: Advanced Regression Techniques competition*. https://www.kaggle.com/c/house-prices-advanced-regression-techniques .  

## Descriptive and Inferential Statistics  

Provide univariate descriptive statistics and appropriate plots for the training data set.  Provide a scatterplot matrix for at least two of the independent variables and the dependent variable. Derive a correlation matrix for any three quantitative variables in the dataset.  Test the hypotheses that the correlations between each pairwise set of variables is 0 and provide an 80% confidence interval.  Discuss the meaning of your analysis.  Would you be worried about familywise error? Why or why not?

### Import Data
```{r}
train_data <- read.csv('train.csv', sep = ',', header = T, stringsAsFactors = F)
test_data <- read.csv("test.csv", sep = ',', header = T, stringsAsFactors = F)
```


### Preview
```{r}
kable(head(train_data, 100)) %>% kable_styling(bootstrap_options = "striped" ,font_size = 11) %>% scroll_box(height = "500px")
```


```{r}
dim(train_data)
```
There are `81` variables and `1460` obvervations in the training dataset.

```{r}
dim(test_data)
```
There are `80` variables and `1459` obvervations in the test dataset.


### Summary
```{r}
summary(train_data)
```

Just to name a few, on average the houses has 2 full bath and the max is 3. Or the average prices for the houses is \$180000 while the most expensive houses cost about \$755000.


### Histogram of all numeric values

```{r}

train_data %>% 
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()

```

### Histogram of Sale Prices
```{r}
SP_hist <- ggplot(train_data, aes(x = SalePrice)) + geom_histogram(fill = "lightblue") + geom_vline(aes(xintercept=mean(SalePrice)), color="darkgreen", linetype="dashed", size=1)

SP_boxplot <- ggplot(train_data, aes(x = "", y = SalePrice)) + geom_boxplot(fill = "lightblue") + xlab("SalePrice")

grid.arrange(SP_hist, SP_boxplot, ncol=2)
```


### Sale Price against Sale Condition.
```{r}
ggplot(train_data, aes(x=SaleCondition, y=SalePrice, fill=SaleCondition)) + geom_bar(stat="identity") + theme_minimal() + scale_fill_brewer(palette="Set3")
```

For better readability:

  - **Normal** Normal Sale
  - **Abnorml**  Abnormal Sale -  trade, foreclosure, short sale
  - **AdjLand**  Adjoining Land Purchase
  - **Alloca**   Allocation - two linked properties with separate deeds, typically condo with a garage unit	
  - **Family**	 Sale between family members
  - **Partial**  Home was not completed when last assessed (associated with New Homes)


### Overall Quality of the houses

```{r}
hquality <- data.frame(table(train_data$OverallQual))
ggplot(hquality, aes(x=Var1, y=Freq)) + geom_bar(stat="identity", color="brown", fill="orange") + xlab("Rate")

```

Houses are sold at average overall quality has a rating of 5. This makes sense because it is very expensive fixing up a house that is very poor and the average buys cannot afford houses rated at 10 because they would be expensive also. The distribution is slightly skewed.

### How old are the houses?

```{r}
year_built <- data.frame(table(train_data$YearBuilt))
ggplot(year_built, aes(x=Var1, y=Freq)) + geom_bar(stat="identity", color="blue", fill="white") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab( "Year")
```

Oldest houses are from the 19 century. Ranges from 1872 - 2010. Most of the houses were built in 2005. 

### House Foundation

Price for houses based on type of foundation.

```{r}
ggplot(train_data, aes(x=Foundation, y=SalePrice, fill = Foundation)) + geom_boxplot()
```

For reference: 

  - **BrkTil**	Brick & Tile
  - **CBlock**	Cinder Block
  - **PConc**	  Poured Contrete	
  - **Slab**	  Slab
  - **Stone**	  Stone
  - **Wood**	  Wood
  
  

### Building Type Sales

```{r}
describeBy(train_data$SalePrice, group = train_data$BldgType)
```

For Reference:

  - **1Fam**	  Single-family Detached	
  - **2FmCon**	Two-family Conversion; originally built as one-family dwelling
  - **Duplx**	  Duplex
  - **TwnhsE**	Townhouse End Unit
  - **TwnhsI**	Townhouse Inside Unit


### Scatterplot Matrix
```{r}
pairs(train_data[,c(5, 19, 39, 63, 81)], pch = 19)

```

We can spot some activity between SalePrice and LotArea or SalePrice and TotalBsmtSF.

### Correlation Matrix

```{r}
corr_mat <- cor(train_data[, c(5, 63, 81)]); corr_mat
corrplot(corr_mat, type="upper", order="hclust")
```



### Test Hypothesis of Correlations

$H_0 = 0 \rightarrow\ There  \ is \ no \ correlation\\H_A \neq 0 \rightarrow \ There \ is \ correlation$

Lot Area and Sale Price

```{r}
cor.test(train_data$LotArea, train_data$SalePrice, conf.level = 0.8)
```

Garage Area and Sale Price
```{r}
cor.test(train_data$GarageArea, train_data$SalePrice, conf.level = 0.8)
```


Lot Area and Garage Area

```{r}
cor.test(train_data$LotArea, train_data$GarageArea, conf.level = 0.8)
```


Based on the results above we will reject the null hypothesis that there is no correlations between the variables above. Also the p-values are very small which makes it safe to say that there is a correlation among each variable (Lot Area, Garage Area and Sale Price).

Family-wise error

The familywise error rate (FWE or FWER) is the probability of ending up with at least one false conclusion in a series of hypothesis tests . In other words, it’s the probability of making at least one Type I Error. 

Let us check the [family-wise error rate](https://www.statisticshowto.datasciencecentral.com/familywise-error-rate/) $\rightarrow FWE ≤ 1 – (1 – \alpha_{IT})^c$ 

Where:

$\alpha_{IT}$ = alpha level for an individual test (e.g. .05), 
c = Number of comparisons.

Alpha Level 5% or 95% confidence interval
```{r}
1 - (1 - 0.05)^3
```

In this case we would not worry much about familywise error as the probability of a type I error is just over 14%, which is low as we only performed three tests.


Alpha Level 20% or 80% confidence interval
```{r}
1 - (1 - 0.2)^3
```

On the other hand, we *would* worry about familywise error as the probability of a type I error is about 49%, which is high considering that we only performed three tests.


***

## Linear Algebra and Correlation

Invert your correlation matrix from above. (This is known as the precision matrix and contains variance inflation factors on the diagonal.) Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix. Conduct LU decomposition on the matrix.

### Invert Correlation Matrix (Precision Matrix)

```{r}
precision_mat <- solve(corr_mat)
precision_mat

```

### Correlation Matrix $\times$ Precision Matrix

```{r}
corr_mat %*% precision_mat

```


### Precision Matrix $\times$ Correlation Matrix

```{r}
mult_mat <- precision_mat %*% corr_mat
mult_mat
```

### LU Decomposition

```{r}
lu_mult_mat <-lu.decomposition(mult_mat)

lu_mult_mat

#proof
lu_mult_mat$L %*% lu_mult_mat$U
```


***

## Calculus-Based Probability & Statistics

Many times, it makes sense to fit a closed form distribution to data.  Select a variable in the Kaggle.com training dataset that is skewed to the right, shift it so that the minimum value is absolutely above zero if necessary.  Then load the MASS package and run fitdistr to fit an exponential probability density function.  (See  https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html ).  Find the optimal value of $\lambda$ for this distribution, and then take 1000 samples from this exponential distribution using this value (e.g., rexp(1000, $\lambda$)).  Plot a histogram and compare it with a histogram of your original variable.   Using the exponential pdf, find the $5^{th}$ and $95^{th}$ percentiles using the cumulative distribution function (CDF).   Also generate a 95% confidence interval from the empirical data, assuming normality.  Finally, provide the empirical 5th percentile and 95th percentile of the data.  Discuss.

The variable I will use is `GrLivArea`.
`
```{r}
#no need to scale as this is the min
min(train_data$GrLivArea)

# fit exponential pdf
fit <- fitdistr(train_data$GrLivArea, densfun = "exponential")

#optimal value for lambda
lambda <- fit$estimate

#1000 samples
exp_hist <- rexp(1000, lambda)

# compare histograms
G1 <- ggplot(train_data, aes(x = GrLivArea)) + geom_histogram(fill = "white", col = "black")
G2 <- ggplot()+ aes(exp_hist) + geom_histogram(fill = "white", col = "black") + xlab("Exp_GrLivArea")

grid.arrange(G1, G2, ncol= 2)

```

The exponential shifts the estimates further to the left with an even longer tail to the right (more spread) compared to the original data. 

```{r}

qexp(c(0.05, 0.95),rate=lambda) ## 5th and 95th percentile

#0.95 confidence interval
CI(train_data$GrLivArea, ci=0.95)

# 5th and 95th percentile of empircal data
quantile(train_data$GrLivArea, c(0.05, 0.95))

```

Based on the results above, at 95% confidence the exponential model would not be a good fit for this data. When looking at the confidence interval for the original data, it does not contain 95% of the data in the exponential model.


***

## Modeling {.tabset .tabset-fade .tabset-pills}

Build some type of multiple regression  model and submit your model to the competition board.  Provide your complete model summary and results with analysis.  Report your Kaggle.com user name and score.

### Missing Data

```{r}
aggr(train_data)

#Impute data using sample mean (Will only work for quantitative variables)
train_data <- complete(mice(data = train_data, m = 1, method = "mean"))

```

### Original Model

```{r}

predSales.lm <- lm(SalePrice ~ MSSubClass +  LotArea + YearBuilt + YearRemodAdd + TotalBsmtSF + GrLivArea + FullBath + BedroomAbvGr + KitchenAbvGr, data = train_data)

summary(predSales.lm)

```

### Final Model

#### Predictors with low significance to the model are removed manually as shown below step-by-step with the `update` function.

Removing `FullBath`...

```{r}
predSales.lm <- update(predSales.lm, .~. - FullBath, data = train_data)
summary(predSales.lm)
```


This model is okay. The p-value is quite small which indicates significance. According to the $Adjusted \ R^2$, the model explains `73.68`% variation of observed values around the mean. This is good. The p-value of the `F-statistic` `511.6` for *DF 8 and 1451* is extremely small which implies that removing variables like `FullBath` gave us a chance to improve the fit of the model significantly.



### Residual Analysis

```{r}
predSales_df <- augment(predSales.lm)

ggplot(predSales_df, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept=0, color = 'brown', size = 1) + ggtitle('Residual vs Fitted')

plot(predSales.lm, 1)

```

The residuals have a curved pattern which shows a small degree of non-linearity. This could be because we did not normalize our data. If you look at the second plot you'll see that there are about three to four extreme values or outliers far above and below the line or far from the model which means that the model did not capture those data points. If we delete them, the model could improve.

```{r}
ggplot(predSales_df, aes(x=.std.resid)) + geom_histogram(aes(y=..density..), bins = 50, colour="black")+
 geom_density(alpha=.2, fill="yellow") + ggtitle('Histogram of Residuals')
```

Based on the histogram above, this is a heavily tailed distribution. We can agree that the histogram seemingly indicates that the distribution is normal. However, we notice that each end of the histogram seems to extend much further than what we usually see in a normal distribution. 


```{r}
qplot(sample =.std.resid, data = predSales_df) + geom_abline()+ ggtitle('Normal Q-Q Plot')
plot(predSales.lm, 2)
```

Here we look to see if the residuals follow normal distribution or not. In our case the residual points follow the dotted line closely except at the tails where the points diverge away from the line curving in opposite directions.


```{r}
plot(predSales.lm, 3)
```

Scale location plot indicates spread of points across predicted values range. A red horizontal line would be ideal and would indicate Homoscedasticity (equal or uniform variance). However, our model shows Heteroskedasticity (non-uniform variance) as the residuals spread out. The higher the price of the houses, the less observations there are. The observations densly populate the line in the low or medium price ranges.


```{r}
plot(predSales.lm, 4)
```

This plot, also known as Cook’s distance, tells us which points have the greatest influence on the regression (leverage points). We see that observations 524, 692 and 1299 have the greatest influence on the model. Perhaps, if we were to remove those outliers our model would be more normal.


### Predict Model

```{r}

mypred <- predict(predSales.lm, subset(test_data, select = c(MSSubClass, LotArea, YearBuilt, TotalBsmtSF, YearRemodAdd, GrLivArea, BedroomAbvGr, KitchenAbvGr)))

kaggle_pred <- data.frame(cbind(test_data$Id, mypred))
colnames(kaggle_pred) <- c("ID", "SalePrice")
kaggle_pred[is.na(kaggle_pred)] <- 0

write.csv(kaggle_pred, file = "PredSub1.csv", row.names = F) #write predictions to file

```

#### [Predictions](https://github.com/javernw/DATA605-Computational-Mathematics/blob/master/PredSub1.csv)
```{r}

kable(head(kaggle_pred, 10)) %>% kable_styling(bootstrap_options = c("striped", "condensed", "responsive"), full_width = F)
```


#### My username is **javernw** and score is `0.19039`

#### The code to this assignment can be found on [MY GITHUB](https://github.com/javernw/DATA605-Computational-Mathematics/blob/master/JWilsonFinal_Project.Rmd)

#### [Link to Youtube presentation](https://youtu.be/iprXrBly4lo)



## References

[Contingency Tables](https://www.datacamp.com/community/tutorials/contingency-tables-r)

[Calculus Probability](https://www.cengage.com/resource_uploads/downloads/1439049254_242719.pdf)

[Multiple Linear Regression](https://www.guru99.com/r-simple-multiple-linear-regression.html)

[Predictive Modeling](https://bookdown.org/egarpor/PM-UC3M/app-nas.html)


